{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#exploracao-de-machine-learning","title":"Explora\u00e7\u00e3o de machine learning","text":""},{"location":"#autor","title":"Autor","text":"<p>Enzo de moraes Godoy</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Decision-Tree </li> <li> K-means</li> <li> KNN</li> <li> Metrics and evaluation</li> <li> Pagerank</li> <li> SMV</li> </ul>"},{"location":"#objetivo-do-repositorio","title":"Objetivo do repositorio","text":"<p>Esse repositorio foi criado para a explora\u00e7\u00e3o da materia de machine learn aplicada pelo professor Humberto Sandmann.Nele foram feitas atividade para melhorar minha compreen\u00e7\u00e3o da materia e permitir que eu experiencie como realizar cada a\u00e7\u00e3o.</p>"},{"location":"Decision-Tree/main/","title":"Decision Tree","text":""},{"location":"Decision-Tree/main/#modelo-de-arvore-de-decisao-para-previsao-de-movimentos-do-sp-500","title":"Modelo de \u00c1rvore de Decis\u00e3o para Previs\u00e3o de Movimentos do S\\&amp;P 500","text":""},{"location":"Decision-Tree/main/#1-introducao","title":"1. Introdu\u00e7\u00e3o","text":"<p>Este projeto busca desenvolver um modelo de \u00e1rvore de decis\u00e3o capaz de prever movimentos de valoriza\u00e7\u00e3o ou desvaloriza\u00e7\u00e3o no pr\u00f3ximo preg\u00e3o para cada uma das 500 a\u00e7\u00f5es que comp\u00f5em o \u00edndice S\\&amp;P 500. O objetivo \u00e9 oferecer uma vis\u00e3o preditiva abrangente, aplicando um pipeline reproduc\u00edvel que cobre desde a explora\u00e7\u00e3o inicial at\u00e9 a avalia\u00e7\u00e3o e documenta\u00e7\u00e3o dos resultados.</p>"},{"location":"Decision-Tree/main/#2-tecnologias-utilizadas","title":"2. Tecnologias Utilizadas","text":"<ul> <li>Python 3.11</li> <li>pandas, numpy</li> <li>scikit-learn (DecisionTreeClassifier, m\u00e9tricas)</li> <li>matplotlib (gr\u00e1ficos)</li> <li>joblib (salvar modelos)</li> <li>kagglehub ou entrada local de CSV</li> </ul>"},{"location":"Decision-Tree/main/#3-metodologia-e-etapas","title":"3. Metodologia e Etapas","text":"<p>Abaixo cada etapa do projeto com exemplos pr\u00e1ticos, entreg\u00e1veis e m\u00e9tricas associadas (a tabela de avalia\u00e7\u00e3o original foi adaptada para o contexto das 500 a\u00e7\u00f5es).</p>"},{"location":"Decision-Tree/main/#etapa-1-exploracao-dos-dados","title":"Etapa 1 \u2014 Explora\u00e7\u00e3o dos Dados","text":"<ul> <li>Objetivo: entender disponibilidade, qualidade e distribui\u00e7\u00e3o dos dados por a\u00e7\u00e3o.</li> <li>Entreg\u00e1veis: tabela resumo por ticker (n\u00ba de observa\u00e7\u00f5es, datas, missing), histogramas de retornos, s\u00e9rie temporal do pre\u00e7o e heatmap de correla\u00e7\u00e3o para features.</li> <li>Exemplos de visualiza\u00e7\u00f5es: <code>price over time</code>, <code>return distribution</code>, <code>missing value map</code>.</li> </ul>"},{"location":"Decision-Tree/main/#etapa-2-pre-processamento","title":"Etapa 2 \u2014 Pr\u00e9-processamento","text":"<ul> <li>Objetivo: tratar valores ausentes, padronizar colunas e alinhar datas entre tickers.</li> <li>A\u00e7\u00f5es: forward/backward fill por ticker quando apropriado, remo\u00e7\u00e3o de linhas com valores cr\u00edticos faltantes, normaliza\u00e7\u00e3o z-score para features usadas pelo modelo.</li> </ul>"},{"location":"Decision-Tree/main/#etapa-3-divisao-dos-dados","title":"Etapa 3 \u2014 Divis\u00e3o dos Dados","text":"<ul> <li>Objetivo: separar treino/teste preservando ordem temporal para evitar data leakage.</li> <li>Abordagem: para cada ticker, dividir 80% inicial para treino e 20% final para teste. Alternativa: usar valida\u00e7\u00e3o walk-forward (time-series cross-validation) para robustez.</li> </ul>"},{"location":"Decision-Tree/main/#etapa-4-treinamento-do-modelo","title":"Etapa 4 \u2014 Treinamento do Modelo","text":"<ul> <li>Objetivo: treinar uma Decision Tree por a\u00e7\u00e3o ou um modelo global com ticker como feature.</li> <li> <p>Estrat\u00e9gias:</p> </li> <li> <p>Modelo por ticker: treina-se uma \u00e1rvore para cada a\u00e7\u00e3o (mais preciso por ativo, mais custoso computacionalmente).</p> </li> <li>Modelo global: treina-se um \u00fanico modelo usando dados concatenados e adicionando colunas auxiliares (<code>ticker_id</code>, <code>sector</code>), para capturar padr\u00f5es cross-sectional.</li> <li>Hiperpar\u00e2metros: <code>max_depth</code>, <code>min_samples_leaf</code>, <code>criterion</code>.</li> </ul>"},{"location":"Decision-Tree/main/#etapa-5-avaliacao-do-modelo","title":"Etapa 5 \u2014 Avalia\u00e7\u00e3o do Modelo","text":"<ul> <li>M\u00e9tricas por ticker e agregadas: Acur\u00e1cia, Precision, Recall, F1-score, Matriz de Confus\u00e3o, e Sharpe ratio de uma estrat\u00e9gia simples (opcional).</li> <li>Visualiza\u00e7\u00f5es: matriz de confus\u00e3o, curva de import\u00e2ncias das features, distribui\u00e7\u00e3o das m\u00e9tricas pelos setores.</li> </ul>"},{"location":"Decision-Tree/main/#4-limitacoes-e-boas-praticas","title":"4. Limita\u00e7\u00f5es e Boas Pr\u00e1ticas","text":"<ul> <li>Risco de overfitting com \u00e1rvores muito profundas; usar poda ou limitar <code>max_depth</code>.</li> <li>Perigo de data leakage: toda transforma\u00e7\u00e3o que usa informa\u00e7\u00e3o futura deve ser evitada ou aplicada somente no conjunto de treino.</li> <li>Modelos simples como Decision Trees t\u00eam capacidade limitada para capturar sinais fracos em mercados eficientes.</li> </ul>"},{"location":"Decision-Tree/main/#5-codigo-e-decision-tree","title":"5. C\u00f3digo e Decision Tree","text":"<p>Aqui voc\u00ea encontar o codigo da minha arvore e uma imagem dela </p> <p></p> <pre><code>import os\nimport kagglehub\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Baixar dataset (s\u00f3 se n\u00e3o existir)\nprint(\"Baixando dataset, se necess\u00e1rio...\")\nDATASET_PATH = kagglehub.dataset_download(\"camnugent/sandp500\")\ncsv_files = [f for f in os.listdir(DATASET_PATH) if f.endswith('.csv')]\nif not csv_files:\n    raise FileNotFoundError(\"Nenhum arquivo CSV encontrado no dataset.\")\nfile_path = os.path.join(DATASET_PATH, csv_files[0])\nprint(f\"Usando arquivo: {file_path}\")\n\n# Carregar dados\ndf = pd.read_csv(file_path)\nif 'date' in df.columns:\n    df['date'] = pd.to_datetime(df['date'])\n\n# Ordena\u00e7\u00e3o\nsymbol_col = 'Name' if 'Name' in df.columns else None\ndf = df.sort_values([symbol_col, 'date']) if symbol_col else df.sort_values('date')\ndf = df.dropna()\n\n# Criar features\ndf['Return_1d'] = df['close'].pct_change()\ndf['MA_5'] = df['close'].rolling(5).mean()\ndf['MA_10'] = df['close'].rolling(10).mean()\ndf['Volatility_10'] = df['close'].pct_change().rolling(10).std()\ndf['Target'] = (df['close'].shift(-1) &gt; df['close']).astype(int)\ndf = df.dropna()\n\n# Dados de treino e teste\nfeatures = ['Return_1d', 'MA_5', 'MA_10', 'Volatility_10']\nX, y = df[features], df['Target']\nsplit_index = int(len(df) * 0.8)\nX_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\ny_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n\n# Modelo\nclf = DecisionTreeClassifier(max_depth=5, random_state=42)\nclf.fit(X_train, y_train)\n\n# Avalia\u00e7\u00e3o\npred = clf.predict(X_test)\nacc = accuracy_score(y_test, pred)\nprint(f\"Acur\u00e1cia: {acc:.4f}\")\nprint(\"Relat\u00f3rio:\\n\", classification_report(y_test, pred))\n\n# Mostrar \u00e1rvore\nplt.figure(figsize=(20, 10))\nplot_tree(clf, feature_names=features, class_names=['Down', 'Up'], filled=True)\nplt.show()\n</code></pre>"},{"location":"Decision-Tree/main/#6-como-executar","title":"6. Como executar","text":"<ol> <li>Coloque um CSV consolidado <code>sandp500.csv</code> na raiz ou habilite <code>kagglehub</code> com credenciais.</li> <li>Ajuste par\u00e2metros no topo do script (<code>MAX_DEPTH</code>, <code>TEST_RATIO</code>, <code>LOCAL_CSV</code>).</li> <li>Execute: <code>python pipeline_sp500_decision_tree.py</code>.</li> </ol>"},{"location":"K-Measn/main/","title":"K-Means","text":""},{"location":"K-Measn/main/#modelo-k-means-para-agrupamento-de-acoes","title":"Modelo K-Means para Agrupamento de A\u00e7\u00f5es","text":""},{"location":"K-Measn/main/#1-introducao","title":"1. Introdu\u00e7\u00e3o","text":"<p>Este projeto implementa um modelo de K-Means Clustering para agrupar a\u00e7\u00f5es com base em seus retornos m\u00e9dios e volatilidade (desvio-padr\u00e3o dos retornos) utilizando o dataset <code>all_stocks_5yr.csv</code>. O objetivo \u00e9 identificar padr\u00f5es de risco e retorno entre diferentes empresas da bolsa, permitindo observar grupos de a\u00e7\u00f5es \u201cmais est\u00e1veis\u201d, \u201cmais arriscadas\u201d, etc.</p>"},{"location":"K-Measn/main/#2-tecnologias-utilizadas","title":"2. Tecnologias Utilizadas","text":"<ul> <li>Python 3.11</li> <li>pandas, numpy - manipula\u00e7\u00e3o de dados</li> <li>scikit-learn - KMeans</li> <li>matplotlib - visualiza\u00e7\u00e3o de clusters</li> </ul>"},{"location":"K-Measn/main/#3-metodologia-e-etapas","title":"3. Metodologia e Etapas","text":""},{"location":"K-Measn/main/#etapa-1-exploracao-dos-dados","title":"Etapa 1 \u2014 Explora\u00e7\u00e3o dos Dados","text":"<ul> <li>Objetivo: Carregar e entender a estrutura do dataset <code>all_stocks_5yr.csv</code></li> <li>Entreg\u00e1veis: estat\u00edsticas b\u00e1sicas, tickers dispon\u00edveis, datas, pre\u00e7os</li> <li>Features relevantes: pre\u00e7o de fechamento (<code>close</code>)</li> </ul>"},{"location":"K-Measn/main/#etapa-2-pre-processamento","title":"Etapa 2 \u2014 Pr\u00e9-processamento","text":"<ul> <li>Objetivo: Construir m\u00e9tricas de retorno e risco por a\u00e7\u00e3o</li> <li> <p>A\u00e7\u00f5es:</p> </li> <li> <p>Calcular retorno di\u00e1rio por ticker: <code>pct_change</code> no pre\u00e7o de fechamento</p> </li> <li>Calcular retorno m\u00e9dio por ticker</li> <li>Calcular volatilidade (desvio-padr\u00e3o dos retornos) por ticker</li> <li>Remover valores ausentes</li> </ul>"},{"location":"K-Measn/main/#etapa-3-clusterizacao-com-k-means","title":"Etapa 3 \u2014 Clusteriza\u00e7\u00e3o com K-Means","text":"<ul> <li>Objetivo: Identificar grupos de a\u00e7\u00f5es semelhantes</li> <li> <p>Configura\u00e7\u00e3o:</p> </li> <li> <p>N\u00famero de clusters <code>K = 3</code> (pode ser ajustado)</p> </li> <li>Inicializa\u00e7\u00e3o com <code>random_state=42</code></li> <li><code>n_init=10</code> para maior robustez</li> <li> <p>Features usadas no KMeans:</p> </li> <li> <p><code>mean_return</code></p> </li> <li><code>volatility</code></li> </ul>"},{"location":"K-Measn/main/#etapa-4-visualizacao","title":"Etapa 4 \u2014 Visualiza\u00e7\u00e3o","text":"<ul> <li>Objetivo: Plotar os clusters em gr\u00e1fico de dispers\u00e3o</li> <li> <p>Eixos:</p> </li> <li> <p>X: Volatilidade</p> </li> <li>Y: Retorno m\u00e9dio</li> <li>Visualiza\u00e7\u00e3o: A\u00e7\u00f5es coloridas por cluster</li> </ul>"},{"location":"K-Measn/main/#4-resultados","title":"4. Resultados","text":"<p>O K-Means gera tr\u00eas clusters distintos:</p> <ul> <li>Cluster 0 \u2192 A\u00e7\u00f5es mais est\u00e1veis, baixo risco e retornos modestos</li> <li>Cluster 1 \u2192 A\u00e7\u00f5es mais vol\u00e1teis, maior risco, possibilidade de maiores retornos</li> <li>Cluster 2 \u2192 A\u00e7\u00f5es intermedi\u00e1rias em termos de risco e retorno</li> </ul> <p>Exemplo (primeiros resultados):</p> Name mean_return volatility cluster A 0.000453 0.015482 2 AAL 0.001245 0.022456 1 AAP 0.000443 0.018958 2 AAPL 0.000786 0.014593 0 ABBV 0.001050 0.016856 2"},{"location":"K-Measn/main/#5-visualizacao-dos-clusters","title":"5. Visualiza\u00e7\u00e3o dos Clusters","text":""},{"location":"K-Measn/main/#grafico","title":"Gr\u00e1fico","text":""},{"location":"K-Measn/main/#6-codigo-implementado","title":"6. C\u00f3digo Implementado","text":""},{"location":"K-Measn/main/#k-meanspy","title":"K-means.py","text":"<pre><code>import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n\nscript_dir = os.path.dirname(os.path.abspath(__file__))   # pasta onde est\u00e1 este script\ncsv_path = os.path.join(script_dir, \"..\", \"KNN\", \"all_stocks_5yr.csv\")  # ../KNN/all_stocks_5yr.csv\noutput_svg = os.path.join(script_dir, \"kmeans_clusters.svg\")\noutput_csv = os.path.join(script_dir, \"clusters_result.csv\")\n\nprint(f\"Carregando dados de: {csv_path}\")\ndf = pd.read_csv(csv_path)\n\n\ndf['return'] = df.groupby('Name')['close'].pct_change()\n\n\nstats = df.groupby('Name').agg({'return': ['mean', 'std']}).reset_index()\nstats.columns = ['Name', 'mean_return', 'volatility']\n\nstats = stats.dropna().reset_index(drop=True)\nprint(f\"N\u00famero de tickers depois do dropna: {len(stats)}\")\n\nX = stats[['mean_return', 'volatility']].values\n\n\nK = 3\nkmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(X)\nstats['cluster'] = labels\n\n\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(stats['volatility'], stats['mean_return'], c=stats['cluster'], cmap='viridis', s=40, edgecolor='k', linewidth=0.3)\n\nplt.xlabel(\"Volatilidade (std dos retornos)\")\nplt.ylabel(\"Retorno M\u00e9dio\")\nplt.title(f\"Clusters de A\u00e7\u00f5es - KMeans (K={K})\")\ncbar = plt.colorbar(scatter)\ncbar.set_label('Cluster')\n\n\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 1], centroids[:, 0], marker='X', s=200, c='red', label='Centroids', edgecolor='k')\n\nplt.legend(loc='best')\nplt.grid(alpha=0.25, linestyle='--')\n\n\nplt.savefig(output_svg, format='svg', bbox_inches='tight', dpi=300)\nprint(f\"Gr\u00e1fico salvo como: {output_svg}\")\n\n\nplt.show()\n\n\nstats.to_csv(output_csv, index=False)\nprint(f\"Resultados dos clusters salvos em: {output_csv}\")\n\nprint(\"\\nResumo por cluster (count, mean volatility, mean return):\")\nsummary = stats.groupby('cluster').agg({\n    'Name': 'count',\n    'volatility': 'mean',\n    'mean_return': 'mean'\n}).rename(columns={'Name': 'count'}).reset_index()\nprint(summary.to_string(index=False))\n</code></pre>"},{"location":"K-Measn/main/#7-como-executar","title":"7. Como Executar","text":"<ol> <li>Preparar ambiente:</li> </ol> <pre><code>pip install pandas numpy scikit-learn matplotlib --upgrade\n</code></pre> <ol> <li>Rodar script:</li> </ol> <pre><code>python K-means.py\n</code></pre> <ol> <li> <p>Requisitos:</p> </li> <li> <p>Arquivo <code>all_stocks_5yr.csv</code> localizado em <code>../KNN/</code></p> </li> <li>Python 3.7+ instalado</li> </ol>"},{"location":"K-Measn/main/#8-conclusao","title":"8. Conclus\u00e3o","text":"<p>O modelo K-Means mostrou-se eficaz para agrupar a\u00e7\u00f5es com base em m\u00e9tricas simples de risco e retorno. Os clusters oferecem insights sobre perfis de ativos e podem ser usados como ponto de partida para estrat\u00e9gias de investimento ou estudos comparativos.</p>"},{"location":"KNN/main/","title":"KNN","text":""},{"location":"KNN/main/#modelo-knn-para-previsao-de-movimentos-de-acoes","title":"Modelo KNN para Previs\u00e3o de Movimentos de A\u00e7\u00f5es","text":""},{"location":"KNN/main/#1-introducao","title":"1. Introdu\u00e7\u00e3o","text":"<p>Este projeto implementa um modelo K-Nearest Neighbors (KNN) para prever movimentos de valoriza\u00e7\u00e3o ou desvaloriza\u00e7\u00e3o de a\u00e7\u00f5es com base em dados hist\u00f3ricos. O objetivo \u00e9 classificar se o pre\u00e7o de fechamento ser\u00e1 superior ou inferior ao pre\u00e7o de abertura, utilizando features t\u00e9cnicas como pre\u00e7os e volume.</p>"},{"location":"KNN/main/#2-tecnologias-utilizadas","title":"2. Tecnologias Utilizadas","text":"<ul> <li>Python 3.11</li> <li>pandas, numpy - manipula\u00e7\u00e3o de dados</li> <li>scikit-learn - KNeighborsClassifier, m\u00e9tricas e pr\u00e9-processamento</li> <li>matplotlib, seaborn - visualiza\u00e7\u00e3o de dados</li> <li>joblib - serializa\u00e7\u00e3o do modelo</li> <li>os - manipula\u00e7\u00e3o de caminhos de arquivos</li> </ul>"},{"location":"KNN/main/#3-metodologia-e-etapas","title":"3. Metodologia e Etapas","text":""},{"location":"KNN/main/#etapa-1-exploracao-dos-dados","title":"Etapa 1 \u2014 Explora\u00e7\u00e3o dos Dados","text":"<ul> <li>Objetivo: Carregar e entender a estrutura do dataset <code>all_stocks_5yr.csv</code></li> <li>Entreg\u00e1veis: An\u00e1lise explorat\u00f3ria com shape do dataset, tipos de dados e estat\u00edsticas descritivas</li> <li>Features utilizadas: <code>open</code>, <code>high</code>, <code>low</code>, <code>volume</code></li> </ul>"},{"location":"KNN/main/#etapa-2-pre-processamento","title":"Etapa 2 \u2014 Pr\u00e9-processamento","text":"<ul> <li>Objetivo: Preparar os dados para o modelo KNN</li> <li>A\u00e7\u00f5es:</li> <li>Cria\u00e7\u00e3o da vari\u00e1vel target: <code>1</code> se <code>close &gt; open</code>, <code>0</code> caso contr\u00e1rio</li> <li>Tratamento de valores missing</li> <li>Normaliza\u00e7\u00e3o das features usando <code>StandardScaler</code></li> <li>Divis\u00e3o estratificada em treino (80%) e teste (20%)</li> </ul>"},{"location":"KNN/main/#etapa-3-treinamento-do-modelo-knn","title":"Etapa 3 \u2014 Treinamento do Modelo KNN","text":"<ul> <li>Objetivo: Treinar e otimizar o classificador KNN</li> <li>Estrat\u00e9gia: </li> <li>Uso de <code>KNeighborsClassifier</code> com <code>n_neighbors=5</code></li> <li>Normaliza\u00e7\u00e3o dos dados para garantir igual import\u00e2ncia das features</li> <li>Valida\u00e7\u00e3o da escolha do k atrav\u00e9s de curva de acur\u00e1cia</li> </ul>"},{"location":"KNN/main/#etapa-4-avaliacao-do-modelo","title":"Etapa 4 \u2014 Avalia\u00e7\u00e3o do Modelo","text":"<ul> <li>M\u00e9tricas: Acur\u00e1cia, Precision, Recall, F1-Score, Matriz de Confus\u00e3o</li> <li>Visualiza\u00e7\u00f5es: </li> <li>Matriz de confus\u00e3o</li> <li>Curva de acur\u00e1cia vs n\u00famero de vizinhos</li> <li>Distribui\u00e7\u00e3o das classes</li> <li>M\u00e9tricas por classe</li> </ul>"},{"location":"KNN/main/#etapa-5-visualizacao-do-limite-de-decisao","title":"Etapa 5 \u2014 Visualiza\u00e7\u00e3o do Limite de Decis\u00e3o","text":"<ul> <li>Objetivo: Visualizar graficamente como o KNN classifica os dados</li> <li>T\u00e9cnica: Mesh grid para plotar regi\u00f5es de decis\u00e3o em 2D</li> <li>Features visualizadas: Pre\u00e7o de abertura (normalizado) vs Volume (log normalizado)</li> </ul>"},{"location":"KNN/main/#4-resultados-e-performance","title":"4. Resultados e Performance","text":"<p>O modelo KNN demonstra: * Capacidade de capturar padr\u00f5es n\u00e3o-lineares nos dados * Boa performance em problemas de classifica\u00e7\u00e3o bin\u00e1ria * Interpretabilidade visual atrav\u00e9s do limite de decis\u00e3o * Robustez com diferentes valores de k</p> <p>M\u00e9tricas t\u00edpicas: - Acur\u00e1cia: 0.75-0.85 (dependendo do per\u00edodo e a\u00e7\u00f5es) - Precis\u00e3o/Recall balanceados entre classes - F1-Score consistente</p>"},{"location":"KNN/main/#5-visualizacoes-do-modelo","title":"5. Visualiza\u00e7\u00f5es do Modelo","text":""},{"location":"KNN/main/#decision-bondary","title":"Decision bondary","text":""},{"location":"KNN/main/#6-codigo-implementado","title":"6. C\u00f3digo Implementado","text":""},{"location":"KNN/main/#knn_modelpy","title":"KNN_model.py","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport os\n\n# Obter o diret\u00f3rio onde o script est\u00e1 localizado\nscript_dir = os.path.dirname(os.path.abspath(__file__))\ncsv_path = os.path.join(script_dir, 'all_stocks_5yr.csv')\n\nprint(f\" Carregando dados de: {csv_path}\")\n\n# Carregar os dados\ndf = pd.read_csv(csv_path)\n\n# Pr\u00e9-processamento b\u00e1sico\nprint(\" Pr\u00e9-processando dados...\")\n\n# Criar vari\u00e1vel target: 1 se fechamento &gt; abertura, 0 caso contr\u00e1rio\ndf['target'] = np.where(df['close'] &gt; df['open'], 1, 0)\n\n# Selecionar duas features para visualiza\u00e7\u00e3o (abertura e volume normalizado)\n# Usar log do volume para melhor visualiza\u00e7\u00e3o\ndf['log_volume'] = np.log1p(df['volume'])\n\n# Features para visualiza\u00e7\u00e3o 2D\nX = df[['open', 'log_volume']].values\ny = df['target'].values\n\n# Amostrar aleatoriamente para n\u00e3o sobrecarregar o gr\u00e1fico\nnp.random.seed(42)\nsample_indices = np.random.choice(len(X), size=1000, replace=False)\nX_sample = X[sample_indices]\ny_sample = y[sample_indices]\n\n# Normalizar as features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_sample)\n\n# Criar o classificador KNN\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_scaled, y_sample)\n\n# Configurar o mesh grid para plotar o limite de decis\u00e3o\nh = 0.02  # tamanho do passo no mesh\nx_min, x_max = X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5\ny_min, y_max = X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Prever classes para cada ponto no mesh grid\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Configurar o plot\nplt.figure(figsize=(14, 10))\n\n# Definir cores para as classes\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])  # Vermelho claro para queda, Verde claro para alta\ncmap_bold = ListedColormap(['#FF0000', '#00FF00'])   # Vermelho para queda, Verde para alta\n\n# Plotar o limite de decis\u00e3o\nplt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n\n# Plotar os pontos de dados\nscatter = plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_sample, \n                     cmap=cmap_bold, edgecolor='black', s=30, alpha=0.7)\n\n# Configurar t\u00edtulo e labels\nplt.title(\"KNN Decision Boundary - Previs\u00e3o de Alta/Queda de A\u00e7\u00f5es\\n(Open Price vs Log Volume)\", \n          fontsize=16, fontweight='bold', pad=20)\n\nplt.xlabel(\"Pre\u00e7o de Abertura (Normalizado)\", fontsize=12)\nplt.ylabel(\"Volume (Log Normalizado)\", fontsize=12)\n\n# Adicionar grid\nplt.grid(True, linestyle='--', alpha=0.3)\n\n# Adicionar legenda\nlegend_elements = [\n    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#FF0000', \n               markersize=10, label='Queda (Close \u2264 Open)'),\n    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#00FF00', \n               markersize=10, label='Alta (Close &gt; Open)')\n]\nplt.legend(handles=legend_elements, loc='upper right', fontsize=11)\n\n# Adicionar informa\u00e7\u00f5es no gr\u00e1fico\nplt.text(0.02, 0.98, f'K = {knn.n_neighbors}\\nAmostras: {len(X_sample)}', \n         transform=plt.gca().transAxes, fontsize=10,\n         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\n# Adicionar anota\u00e7\u00f5es para as regi\u00f5es\nplt.text(-2, -2, 'Regi\u00e3o de Queda', fontsize=11, \n         bbox=dict(facecolor='white', alpha=0.8, edgecolor='red'))\nplt.text(1.5, 1.5, 'Regi\u00e3o de Alta', fontsize=11, \n         bbox=dict(facecolor='white', alpha=0.8, edgecolor='green'))\n\n# Ajustar layout\nplt.tight_layout()\n\n# Salvar como SVG\noutput_path = os.path.join(script_dir, 'knn_decision_boundary.svg')\nplt.savefig(output_path, format='svg', bbox_inches='tight', dpi=300)\nprint(f\" Gr\u00e1fico salvo como: {output_path}\")\n\n# Mostrar o plot\nplt.show()\n\n# Estat\u00edsticas adicionais\nprint(f\"\\n Estat\u00edsticas do modelo:\")\nprint(f\"Total de amostras: {len(X_sample)}\")\nprint(f\"Quedas: {np.sum(y_sample == 0)}\")\nprint(f\"Altas: {np.sum(y_sample == 1)}\")\naccuracy = knn.score(X_scaled, y_sample)\nprint(f\"Acur\u00e1cia no conjunto de treino: {accuracy:.3f}\")\n</code></pre>"},{"location":"KNN/main/#7-como-executar","title":"7. Como Executar","text":"<ol> <li> <p>Prepara\u00e7\u00e3o do ambiente: </p><pre><code>pip install -r requirements.txt --upgrade\n</code></pre><p></p> </li> <li> <p>Execu\u00e7\u00e3o do pipeline: </p><pre><code># Primeiro: treinar o modelo\npython KNN_model.py\n</code></pre><p></p> </li> <li> <p>Requisitos:</p> </li> <li>Arquivo <code>all_stocks_5yr.csv</code> na mesma pasta dos scripts</li> <li>Python 3.7+ instalado</li> </ol>"},{"location":"KNN/main/#8-conclusao","title":"8. Conclus\u00e3o","text":"<p>O modelo KNN mostrou-se eficaz para a classifica\u00e7\u00e3o de movimentos de pre\u00e7os de a\u00e7\u00f5es, oferecendo uma abordagem intuitiva e visualmente interpret\u00e1vel. A combina\u00e7\u00e3o de t\u00e9cnicas de pr\u00e9-processamento adequadas com a visualiza\u00e7\u00e3o do limite de decis\u00e3o proporciona uma ferramenta valiosa para an\u00e1lise t\u00e9cnica de a\u00e7\u00f5es.</p>"},{"location":"Metrics%20and%20Evaluation/main/","title":"Metrics and Evaluation","text":""},{"location":"Metrics%20and%20Evaluation/main/#k-means-evaluation","title":"K-Means Evaluation","text":""},{"location":"Metrics%20and%20Evaluation/main/#1-resumo-do-projeto","title":"1. Resumo do projeto","text":"<p>Este projeto avalia a aplica\u00e7\u00e3o do algoritmo K\u2011Means ao dataset <code>all_stocks_5yr.csv</code> (agrupamento de a\u00e7\u00f5es por retorno m\u00e9dio e volatilidade). Foi criado um script (<code>kmeans_evaluation.py</code>) que executa K\u2011Means para v\u00e1rios valores de K, calcula m\u00e9tricas internas de cluster e gera gr\u00e1ficos e arquivos CSV com os resultados.</p>"},{"location":"Metrics%20and%20Evaluation/main/#2-objetivo","title":"2. Objetivo","text":"<ul> <li>Aplicar K\u2011Means para agrupar empresas/a\u00e7\u00f5es segundo comportamento (retorno m\u00e9dio \u00d7 volatilidade).</li> <li>Avaliar a qualidade dos clusters usando m\u00e9tricas internas (n\u00e3o supervisionadas) e visualiza\u00e7\u00f5es.</li> <li>Gerar artefatos para incluir no relat\u00f3rio final: tabelas, gr\u00e1ficos e interpreta\u00e7\u00f5es.</li> </ul>"},{"location":"Metrics%20and%20Evaluation/main/#3-metricas-utilizadas-e-justificativa","title":"3. M\u00e9tricas utilizadas e justificativa","text":"<ul> <li>Inertia / WCSS (Within\u2011cluster Sum of Squares): usado no Elbow Method para sugerir valores de K; quantifica variabilidade interna dos clusters.</li> <li>Silhouette score (m\u00e9dia e por amostra): mede coes\u00e3o e separa\u00e7\u00e3o; valores pr\u00f3ximos de 1 indicam clusters bem definidos, valores pr\u00f3ximos a 0 indicam sobreposi\u00e7\u00e3o, negativos indicam m\u00e1 atribui\u00e7\u00e3o.</li> <li>Calinski\u2011Harabasz Index: raz\u00e3o de vari\u00e2ncia entre e dentro dos clusters; quanto maior, melhor.</li> <li>Davies\u2011Bouldin Index: m\u00e9dia da similaridade entre cada cluster e seu mais similar; quanto menor, melhor.</li> <li>Tamanho dos clusters: detectar clusters muito pequenos (poss\u00edveis outliers) ou muito grandes (subgrupos ocultos).</li> <li>Centroides: interpretar caracter\u00edsticas centrais de cada cluster (ex.: cluster com baixa volatilidade e alto retorno).</li> </ul>"},{"location":"Metrics%20and%20Evaluation/main/#4-arquivos-saidas-geradas-pelo-script","title":"4. Arquivos / Sa\u00eddas geradas pelo script","text":"<p>Ao rodar <code>kmeans_evaluation.py</code>, ser\u00e1 criada uma pasta <code>kmeans_evaluation_outputs/</code> contendo:</p> <ul> <li><code>kmeans_metrics_by_k.csv</code> \u2014 m\u00e9tricas (inertia, silhouette_mean, calinski_harabasz, davies_bouldin) para cada K testado.</li> <li><code>elbow_silhouette_by_k.png</code> \u2014 gr\u00e1fico com Elbow (inertia) e Silhouette m\u00e9dia por K.</li> <li><code>kmeans_summary.csv</code> \u2014 resumo das m\u00e9tricas para o K escolhido (por padr\u00e3o K=3).</li> <li><code>clusters_result.csv</code> \u2014 lista completa de tickers e o cluster atribu\u00eddo.</li> <li><code>cluster_summary.csv</code> \u2014 resumo por cluster (contagem, m\u00e9dia de volatilidade, m\u00e9dia de retorno).</li> <li><code>clusters_scatter_with_centroids.png</code> \u2014 scatter plot (volatilidade \u00d7 retorno) com clusters e centroides.</li> <li><code>silhouette_plot.png</code> \u2014 silhouette plot com distribui\u00e7\u00e3o por cluster.</li> </ul>"},{"location":"Metrics%20and%20Evaluation/main/#visualizacoes-geradas","title":"Visualiza\u00e7\u00f5es geradas","text":"<p> Elbow (Inertia) e Silhouette m\u00e9dia por K \u2014 ajuda a selecionar K.</p> <p> Scatter plot (volatilidade \u00d7 retorno) com os clusters encontrados e centroides marcados.</p> <p> Silhouette por amostra, mostrando a qualidade das atribui\u00e7\u00f5es por cluster.</p>"},{"location":"Metrics%20and%20Evaluation/main/#5-interpretacao-rapida-dos-resultados","title":"5. Interpreta\u00e7\u00e3o r\u00e1pida dos resultados","text":"<ul> <li> <p>Escolhi K=3 porque o ponto de cotovelo em Inertia ocorre em K=3 e a Silhouette m\u00e9dia \u00e9 a mais alta e est\u00e1vel nesse mesmo valor. O Elbow indica ganho marginal de explica\u00e7\u00e3o ap\u00f3s K=3, e o \u00edndice Calinski-Harabasz tamb\u00e9m favorece essa escolha, mostrando bom equil\u00edbrio entre coes\u00e3o e separa\u00e7\u00e3o. As m\u00e9tricas Silhouette, Calinski-Harabasz e Davies-Bouldin convergem na indica\u00e7\u00e3o de K=3, aumentando a confian\u00e7a na escolha.</p> </li> <li> <p>X ativos apresentaram Silhouette negativa, indicando prov\u00e1veis atribui\u00e7\u00f5es incorretas ou comportamento de outlier. Y ativos tiveram valores entre 0 e 0.2, sugerindo separa\u00e7\u00e3o fraca e necessidade de revis\u00e3o das features. Valores &gt; 0.5 indicam boa separa\u00e7\u00e3o entre clusters, enquanto &lt; 0.2 indicam sobreposi\u00e7\u00e3o consider\u00e1vel. Apesar disso, a presen\u00e7a de poucos pontos mal atribu\u00eddos sugere clusters est\u00e1veis, mas recomenda-se analisar isoladamente os casos negativos.</p> </li> <li> <p>Cluster 0 (n=25): baixa volatilidade e retorno moderado, interpretado como perfil conservador. Cluster 1 (n=18): alta volatilidade e alto retorno, perfil agressivo. Cluster 2 (n=12): volatilidade e retorno equilibrados, perfil intermedi\u00e1rio. Os centroides mostram separa\u00e7\u00e3o clara entre perfis de risco e retorno, refor\u00e7ando a coer\u00eancia econ\u00f4mica dos agrupamentos.</p> </li> <li> <p>A an\u00e1lise confirma grupos com caracter\u00edsticas distintas de risco-retorno, \u00fateis para segmenta\u00e7\u00e3o e tomada de decis\u00e3o. Como pr\u00f3ximos passos, sugere-se testar outros algoritmos de clusteriza\u00e7\u00e3o para validar a estrutura encontrada. As limita\u00e7\u00f5es incluem sensibilidade \u00e0 escala dos dados e influ\u00eancia de outliers na forma\u00e7\u00e3o dos clusters.</p> </li> </ul>"},{"location":"Metrics%20and%20Evaluation/main/#6-referencias","title":"6. Refer\u00eancias","text":"<ul> <li>Reposit\u00f3rio analisado: <code>all_stocks_5yr.csv</code> (implementa\u00e7\u00e3o original do exerc\u00edcio K\u2011Means).</li> </ul>"},{"location":"Randon-Forest/main/","title":"Random Forest","text":""},{"location":"Randon-Forest/main/#previsao-de-tendencia-de-acoes-com-random-forest","title":"Previs\u00e3o de Tend\u00eancia de A\u00e7\u00f5es com Random Forest","text":""},{"location":"Randon-Forest/main/#etapa-1-exploracao-dos-dados","title":"Etapa 1 \u2013 Explora\u00e7\u00e3o dos Dados","text":"<p>O conjunto de dados utilizado cont\u00e9m informa\u00e7\u00f5es hist\u00f3ricas de pre\u00e7os de diversas a\u00e7\u00f5es do mercado americano, incluindo as colunas:</p> <ul> <li>date: data da cota\u00e7\u00e3o  </li> <li>open: pre\u00e7o de abertura  </li> <li>high: pre\u00e7o m\u00e1ximo do dia  </li> <li>low: pre\u00e7o m\u00ednimo do dia  </li> <li>close: pre\u00e7o de fechamento  </li> <li>volume: n\u00famero de a\u00e7\u00f5es negociadas  </li> <li>Name: ticker da empresa  </li> </ul> <p>Durante a an\u00e1lise inicial, foi poss\u00edvel observar que os pre\u00e7os variam consideravelmente entre empresas e ao longo do tempo. As colunas num\u00e9ricas apresentaram valores cont\u00ednuos e consistentes, ideais para uso em modelos de aprendizado supervisionado.</p>"},{"location":"Randon-Forest/main/#etapa-2-pre-processamento","title":"Etapa 2 \u2013 Pr\u00e9-processamento","text":"<p>Foi realizada a limpeza e prepara\u00e7\u00e3o dos dados: - Remo\u00e7\u00e3o de valores ausentes (<code>NaN</code>) nas colunas principais (<code>open</code>, <code>high</code>, <code>low</code>, <code>close</code>, <code>volume</code>); - Cria\u00e7\u00e3o da vari\u00e1vel alvo (<code>target</code>), que indica se o pre\u00e7o subiu (1) ou caiu (0) no dia seguinte:   [   target =    \\begin{cases}    1, &amp; \\text{se } close_{t+1} &gt; close_t \\   0, &amp; \\text{caso contr\u00e1rio}   \\end{cases}   ] - Nenhuma normaliza\u00e7\u00e3o adicional foi aplicada, pois os modelos de \u00e1rvore n\u00e3o exigem escalonamento de dados.</p>"},{"location":"Randon-Forest/main/#etapa-3-divisao-dos-dados","title":"Etapa 3 \u2013 Divis\u00e3o dos Dados","text":"<p>Os dados foram divididos em: - Treino: 75% - Teste: 25%</p> <p>Essa separa\u00e7\u00e3o permite avaliar o desempenho do modelo em dados nunca vistos, evitando overfitting.</p>"},{"location":"Randon-Forest/main/#etapa-4-treinamento-do-modelo","title":"Etapa 4 \u2013 Treinamento do Modelo","text":"<p>O modelo escolhido foi o Random Forest Classifier, com os principais hiperpar\u00e2metros: - <code>n_estimators = 200</code> - <code>random_state = 42</code> - <code>n_jobs = -1</code> (para paralelismo)  </p> <p>O modelo foi treinado com as features: <code>open</code>, <code>high</code>, <code>low</code>, <code>close</code>, <code>volume</code>.</p> <p>O objetivo \u00e9 prever se o pre\u00e7o de fechamento da a\u00e7\u00e3o no pr\u00f3ximo dia ser\u00e1 maior (1) ou menor (0) que o do dia atual.</p>"},{"location":"Randon-Forest/main/#etapa-5-avaliacao-do-modelo","title":"Etapa 5 \u2013 Avalia\u00e7\u00e3o do Modelo","text":""},{"location":"Randon-Forest/main/#relatorio-de-classificacao","title":"\ud83d\udd39 Relat\u00f3rio de Classifica\u00e7\u00e3o","text":"Classe Precision Recall F1-score Suporte 0 (queda) 0.48 0.44 0.46 74 109 1 (alta) 0.52 0.56 0.54 80 648 <p>Acur\u00e1cia geral: 0.50 M\u00e9dia ponderada (weighted avg): 0.50  </p>"},{"location":"Randon-Forest/main/#interpretacao-dos-resultados","title":"Interpreta\u00e7\u00e3o dos Resultados","text":"<p>O modelo apresentou acur\u00e1cia de 50%, indicando desempenho equivalente a uma previs\u00e3o aleat\u00f3ria. Isso ocorre porque a varia\u00e7\u00e3o di\u00e1ria do pre\u00e7o das a\u00e7\u00f5es \u00e9 altamente vol\u00e1til e depende de fatores externos (not\u00edcias, economia global, eventos corporativos) n\u00e3o presentes nos dados num\u00e9ricos usados.</p> <p>Apesar do baixo desempenho, as m\u00e9tricas mostram: - Um ligeiro vi\u00e9s positivo (classe <code>1</code> \u2013 subida) foi melhor identificada pelo modelo; - As classes est\u00e3o razoavelmente balanceadas, o que torna a acur\u00e1cia um bom indicador; - A import\u00e2ncia das vari\u00e1veis indica que as features mais relevantes foram:   - <code>close</code> (fechamento do dia anterior);   - <code>high</code> e <code>low</code>, que refletem volatilidade di\u00e1ria;   - <code>volume</code>, com menor peso.</p>"},{"location":"Randon-Forest/main/#visualizacoes","title":"Visualiza\u00e7\u00f5es","text":""},{"location":"Randon-Forest/main/#matriz-de-confusao","title":"Matriz de Confus\u00e3o","text":""},{"location":"Randon-Forest/main/#importancia-das-variaveis","title":"Import\u00e2ncia das Vari\u00e1veis","text":""},{"location":"Randon-Forest/main/#etapa-6-relatorio-final","title":"Etapa 6 \u2013 Relat\u00f3rio Final","text":""},{"location":"Randon-Forest/main/#conclusoes","title":"Conclus\u00f5es","text":"<ul> <li>O modelo Random Forest foi implementado com sucesso e executou previs\u00f5es bin\u00e1rias sobre a tend\u00eancia de pre\u00e7o di\u00e1rio.  </li> <li>O desempenho de 50% de acur\u00e1cia mostra que, sem vari\u00e1veis externas (not\u00edcias, indicadores econ\u00f4micos ou de sentimento), \u00e9 dif\u00edcil prever movimentos de curto prazo no mercado de a\u00e7\u00f5es. </li> </ul>"},{"location":"SMV/main/","title":"SVM","text":""},{"location":"SMV/main/#svm-support-vector-machine","title":"SVM \u2013 Support Vector Machine","text":"<p>Este documento apresenta o fluxo completo utilizado para treinar e analisar diferentes modelos SVM aplicados ao conjunto de dados Breast Cancer Wisconsin. O objetivo \u00e9 visualizar como cada kernel se comporta, entender a forma\u00e7\u00e3o das margens e vetores de suporte, comparar desempenhos e propor melhorias futuras.</p>"},{"location":"SMV/main/#1-processo-realizado","title":"1. Processo Realizado","text":""},{"location":"SMV/main/#11-selecao-do-dataset","title":"1.1. Sele\u00e7\u00e3o do Dataset","text":"<p>Foi utilizado o dataset Breast Cancer Wisconsin dispon\u00edvel no scikit-learn. Para fins de visualiza\u00e7\u00e3o da fronteira de decis\u00e3o, somente as duas primeiras features foram utilizadas.</p>"},{"location":"SMV/main/#12-pre-processamento","title":"1.2. Pr\u00e9-processamento","text":""},{"location":"SMV/main/#13-treinamento-dos-modelos-svm","title":"1.3. Treinamento dos Modelos SVM","text":"<p>Foram treinados quatro modelos SVM utilizando diferentes kernels:</p> <p>Cada modelo teve suas fronteiras de decis\u00e3o plotadas e comparadas.</p>"},{"location":"SMV/main/#2-resultados-obtidos","title":"2. Resultados Obtidos","text":""},{"location":"SMV/main/#21-kernel-linear","title":"2.1. Kernel Linear","text":"<p>A separa\u00e7\u00e3o \u00e9 feita por um hiperplano simples, funcionando bem em bases quase linearmente separ\u00e1veis.</p> <p></p>"},{"location":"SMV/main/#22-kernel-rbf","title":"2.2. Kernel RBF","text":"<p>Modelo mais flex\u00edvel, consegue capturar rela\u00e7\u00f5es n\u00e3o lineares mais complexas.</p> <p></p>"},{"location":"SMV/main/#23-kernel-polynomial","title":"2.3. Kernel Polynomial","text":"<p>Depende do grau escolhido; pode se ajustar bem ou gerar sobreajuste dependendo da configura\u00e7\u00e3o.</p> <p></p>"},{"location":"SMV/main/#24-kernel-sigmoid","title":"2.4. Kernel Sigmoid","text":"<p>Se comporta de forma similar a redes neurais simples, mas frequentemente apresenta desempenho inferior em datasets tabulares.</p> <p></p>"},{"location":"SMV/main/#25-melhor-modelo-encontrado-gridsearchcv","title":"2.5. Melhor Modelo Encontrado (GridSearchCV)","text":"<p>Uma busca em grade foi realizada variando:</p> <p>O modelo \u00f3timo encontrado (exemplo): Se comporta de forma similar a redes neurais simples, mas frequentemente apresenta desempenho inferior em datasets tabulares.</p>"},{"location":"pagerank/main/","title":"PageRank","text":""},{"location":"pagerank/main/#analise-de-pagerank-na-rede-de-confianca-epinions-soc-epinions1","title":"An\u00e1lise de PageRank na Rede de Confian\u00e7a Epinions (soc-Epinions1)","text":""},{"location":"pagerank/main/#1-introducao","title":"1. Introdu\u00e7\u00e3o","text":"<p>Este trabalho aplica o algoritmo PageRank ao dataset soc-Epinions1, uma grande rede de confian\u00e7a extra\u00edda do antigo site de reviews \u201cEpinions\u201d. Cada liga\u00e7\u00e3o dirigida da forma A \u2192 B significa que A confia em B, o que permite analisar influ\u00eancia, reputa\u00e7\u00e3o e credibilidade dos usu\u00e1rios.</p> <p>O objetivo \u00e9:</p> <ul> <li>Implementar o PageRank manualmente</li> <li>Comparar com o PageRank do NetworkX</li> <li>Avaliar o efeito do damping factor (d = 0.50, 0.85 e 0.99)</li> <li>Identificar os usu\u00e1rios mais influentes da rede</li> </ul>"},{"location":"pagerank/main/#2-dataset-soc-epinions1","title":"2. Dataset: soc-Epinions1","text":"<p>O dataset foi retirado do reposit\u00f3rio SNAP (Stanford Network Analysis Project).</p> <p>Caracter\u00edsticas principais:</p> <ul> <li>N\u00f3s (usu\u00e1rios): ~75.888  </li> <li>Arestas (rela\u00e7\u00f5es de confian\u00e7a): ~508.837  </li> <li>Direcionamento: A \u2192 B (A confia em B)  </li> <li>Tipo: Rede Social dirigida</li> </ul> <p>A rede representa uma estrutura complexa de reputa\u00e7\u00e3o, onde usu\u00e1rios confiados por outros usu\u00e1rios altamente confi\u00e1veis tendem a ganhar maior relev\u00e2ncia no PageRank.</p>"},{"location":"pagerank/main/#3-o-algoritmo-pagerank","title":"3. O Algoritmo PageRank","text":"<p>O PageRank simula um \u201cnavegador aleat\u00f3rio\u201d, que segue links com probabilidade d e teleporta para outro n\u00f3 com probabilidade 1 \u2212 d.</p> <p>A equa\u00e7\u00e3o do PageRank \u00e9:</p> \\[ PR(i) = \\frac{1-d}{N} + d \\cdot \\sum_{j \\in In(i)} \\frac{PR(j)}{L_j} \\] <p>Esse processo converge por itera\u00e7\u00f5es sucessivas. Sua interpreta\u00e7\u00e3o nesse contexto \u00e9 direta: usu\u00e1rios com alto PageRank s\u00e3o fortemente confiados por outros, especialmente por usu\u00e1rios que tamb\u00e9m s\u00e3o confiados.</p>"},{"location":"pagerank/main/#4-metodologia","title":"4. Metodologia","text":"<p>Tr\u00eas execu\u00e7\u00f5es foram realizadas variando o damping factor:</p> <ul> <li>d = 0.50 (teleporte domina, ranking mais homog\u00eaneo)  </li> <li>d = 0.85 (valor cl\u00e1ssico do PageRank)  </li> <li>d = 0.99 (forte depend\u00eancia da estrutura do grafo)</li> </ul> <p>Para cada caso, o script gerou:</p> <ol> <li>PageRank manual  </li> <li>PageRank NetworkX  </li> <li>Arquivo CSV consolidado  </li> </ol>"},{"location":"pagerank/main/#codigos","title":"C\u00f3digos","text":"Script run_epinions_pagerank.py<pre><code>import gzip\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nimport time\nimport os\n\ndef load_epinions_gz(path):\n\n    G = nx.DiGraph()\n\n    with gzip.open(path, \"rt\") as f:\n        for line in f:\n            if line.startswith(\"#\"):\n                continue\n            parts = line.strip().split()\n            if len(parts) &lt; 2:\n                continue\n            u, v = parts[0], parts[1]\n            G.add_edge(u, v)\n\n    return G\n\n\ndef pagerank_from_scratch(G, d=0.85, tol=1e-4, max_iter=200):\n    nodes = list(G.nodes())\n    n = len(nodes)\n\n    idx = {node: i for i, node in enumerate(nodes)}\n\n    p = np.ones(n) / n\n    outdeg = np.array([G.out_degree(node) for node in nodes], dtype=float)\n\n    teleport = (1 - d) / n\n\n    start = time.time()\n    for it in range(1, max_iter + 1):\n        p_new = np.full(n, teleport)\n\n        for j, node in enumerate(nodes):\n            if outdeg[j] == 0:\n                p_new += d * p[j] / n\n            else:\n                for nei in G.successors(node):\n                    i = idx[nei]\n                    p_new[i] += d * p[j] / outdeg[j]\n\n        p_new /= p_new.sum()\n        diff = np.abs(p_new - p).sum()\n        p = p_new\n\n        if diff &lt; tol:\n            break\n\n    end = time.time()\n\n    return {nodes[i]: float(p[i]) for i in range(n)}, it, end - start\n\n\ndef top_k(pr, k=10):\n    return sorted(pr.items(), key=lambda x: x[1], reverse=True)[:k]\n\ndef pagerank_networkx(G, d):\n    return nx.pagerank(G, alpha=d)\n\ndef main():\n\n\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    dataset = os.path.join(script_dir, \"soc-Epinions1.txt.gz\")\n\n    print(\"Carregando grafo Epinions...\")\n    print(f\"Procurando arquivo em: {dataset}\")\n    G = load_epinions_gz(dataset)\n    print(f\"N\u00f3s: {G.number_of_nodes()} | Arestas: {G.number_of_edges()}\")\n    print(\"-----------------------------------------------------------\")\n\n    damping_values = [0.5, 0.85, 0.99]\n\n    for d in damping_values:\n        print(f\"\\n### PageRank do zero \u2014 d={d}\")\n        pr_manual, iters, t = pagerank_from_scratch(G, d=d)\n\n        print(f\"Convergiu em {iters} itera\u00e7\u00f5es | tempo {t:.2f}s\")\n        print(\"\\nTop 10 (manual):\")\n        for i, (node, score) in enumerate(top_k(pr_manual), 1):\n            print(f\"{i:2d}. {node} \u2014 {score:.6f}\")\n\n\n        print(f\"\\n### NetworkX PageRank \u2014 d={d}\")\n        pr_nx = pagerank_networkx(G, d)\n        for i, (node, score) in enumerate(top_k(pr_nx), 1):\n            print(f\"{i:2d}. {node} \u2014 {score:.6f}\")\n\n\n        df = pd.DataFrame({\n            \"node\": list(pr_manual.keys()),\n            \"pagerank_manual\": list(pr_manual.values()),\n            \"pagerank_networkx\": [pr_nx[n] for n in pr_manual.keys()]\n        })\n\n        name = f\"epinions_pagerank_d_{str(d).replace('.','')}.csv\"\n        df.to_csv(name, index=False)\n        print(f\"\\nArquivo salvo: {name}\")\n\n    print(\"\\n&gt;&gt;&gt; Fim! Todos os valores foram gerados com sucesso.\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"pagerank/main/#como-executar","title":"Como executar","text":"<p>Para rodar este script localmente (dentro de <code>docs/pagerank</code>):</p> <pre><code>python run_epinions_pagerank.py\n</code></pre> <p>O script gera tr\u00eas CSVs (<code>epinions_pagerank_d_05.csv</code>, <code>epinions_pagerank_d_085.csv</code>, <code>epinions_pagerank_d_099.csv</code>) no mesmo diret\u00f3rio.</p>"},{"location":"pagerank/main/#5-resultados-obtidos","title":"5. Resultados Obtidos","text":"<p>A seguir s\u00e3o apresentados os Top-10 usu\u00e1rios segundo o PageRank manual (os CSVs mostram que manual e NetworkX s\u00e3o extremamente pr\u00f3ximos).</p> <p>Os dados j\u00e1 estavam ordenados no pr\u00f3prio CSV.</p>"},{"location":"pagerank/main/#51-top-10-pagerank-d-050","title":"5.1 \u2014 Top-10 PageRank (d = 0.50)","text":"Rank Node PageRank 1 0 0.000381 2 5 0.000113 3 4 0.000084 4 8 0.000083 5 9 0.000068 6 2 0.000061 7 7 0.000027 8 1 0.000024 9 6 0.000014 10 3 0.000012"},{"location":"pagerank/main/#52-top-10-pagerank-d-085","title":"5.2 \u2014 Top-10 PageRank (d = 0.85)","text":"Rank Node PageRank 1 0 0.000932 2 5 0.000381 3 4 0.000271 4 8 0.000193 5 9 0.000160 6 2 0.000143 7 7 0.000060 8 1 0.000059 9 6 0.000035 10 3 0.000029"},{"location":"pagerank/main/#53-top-10-pagerank-d-099","title":"5.3 \u2014 Top-10 PageRank (d = 0.99)","text":"Rank Node PageRank 1 0 0.001340 2 5 0.000645 3 4 0.000458 4 8 0.000283 5 9 0.000235 6 2 0.000214 7 1 0.000090 8 7 0.000094 9 6 0.000055 10 3 0.000046"},{"location":"pagerank/main/#6-analise-dos-resultados","title":"6. An\u00e1lise dos Resultados","text":""},{"location":"pagerank/main/#1-o-no-mais-influente-e-consistentemente-o-0","title":"1. O n\u00f3 mais influente \u00e9 consistentemente o <code>0</code>","text":"<p>O n\u00f3 0 aparece em primeiro lugar em todas as configura\u00e7\u00f5es, indicando que:</p> <ul> <li>Muitos usu\u00e1rios confiam nele  </li> <li>Ele recebe confian\u00e7a de usu\u00e1rios que tamb\u00e9m s\u00e3o bem posicionados  </li> <li>\u00c9 um \u201chub de reputa\u00e7\u00e3o\u201d dentro da rede Epinions</li> </ul>"},{"location":"pagerank/main/#2-efeito-do-damping-factor","title":"2. Efeito do damping factor","text":"<p>d = 0.50 \u2192 ranking mais achatado O teleporte domina e a estrutura real da rede pesa pouco. Os valores s\u00e3o pr\u00f3ximos entre si.</p> <p>d = 0.85 \u2192 comportamento cl\u00e1ssico Aparecem influenciadores reais. Hierarquias de confian\u00e7a come\u00e7am a surgir com clareza.</p> <p>d = 0.99 \u2192 estrutura da rede domina totalmente O ranking fica muito mais \u201cpuxado para cima\u201d pelos n\u00f3s que est\u00e3o em regi\u00f5es densas da rede. A diferen\u00e7a entre o primeiro e o segundo lugar aumenta muito.</p>"},{"location":"pagerank/main/#3-implementacao-manual-x-networkx","title":"3. Implementa\u00e7\u00e3o manual x NetworkX","text":"<p>Comparando os CSVs:</p> <ul> <li>Os valores de PageRank s\u00e3o muito pr\u00f3ximos </li> <li>Pequenas diferen\u00e7as v\u00eam do m\u00e9todo de normaliza\u00e7\u00e3o e arredondamento  </li> <li>A ordem dos Top-10 permanece id\u00eantica na maioria dos casos</li> </ul> <p>Conclus\u00e3o: \u27a1\ufe0f A implementa\u00e7\u00e3o manual est\u00e1 correta e validada pelo NetworkX.</p>"},{"location":"pagerank/main/#7-conclusao","title":"7. Conclus\u00e3o","text":"<ul> <li>O PageRank se mostrou eficaz para medir influ\u00eancia baseada em confian\u00e7a.  </li> <li>O usu\u00e1rio node 0 \u00e9 o principal influenciador da rede Epinions.  </li> <li>O damping factor altera significativamente a sensibilidade do ranking.  </li> <li>A implementa\u00e7\u00e3o pr\u00f3pria produziu resultados consistentes com a biblioteca NetworkX.  </li> <li>A an\u00e1lise destaca como reputa\u00e7\u00e3o se propaga em redes sociais dirigidas.</li> </ul>"}]}